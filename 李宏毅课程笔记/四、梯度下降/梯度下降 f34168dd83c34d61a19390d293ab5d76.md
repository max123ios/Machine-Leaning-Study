# 梯度下降

## 1、知识回顾—梯度下降的过程

![Untitled](%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%20f34168dd83c34d61a19390d293ab5d76/Untitled.png)

- 随机初始化参数
- 计算参数的梯度
- 沿梯度下降的方向更新参数
- 重新得到梯度
- 重复上面的步骤进行参数更新，直到找到最好的模型参数

## 2、学习率的调整

![Untitled](%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%20f34168dd83c34d61a19390d293ab5d76/Untitled%201.png)

学习率对模型训练的影响非常大，过小可能导致拟合速度减少，过大，可能没法拟合到最低点，巨大可能直接爆炸，可通过loss的曲线变化图，来判断学习率选取的问题

- 自适应学习率

        learning-rate 调整的大方向是：开始时选择大学习率让模型调整的快，后来时，减小学习率，因为已经接近最优化的底部，要缓慢调整。

![Untitled](%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%20f34168dd83c34d61a19390d293ab5d76/Untitled%202.png)

！重要信息：学习率做不到，一个学习率适应于所有参数。

最好的情况是，一个参数有一个自己的学习率。

- Adagrad

![Untitled](%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%20f34168dd83c34d61a19390d293ab5d76/Untitled%203.png)

        

![Untitled](%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%20f34168dd83c34d61a19390d293ab5d76/Untitled%204.png)

Adagrad学习率的更新策略，每步的学习步长会除以其平方和开根，这样就做到了每一个参数有不同的学习率。下图是这个算法的最终式。

![Untitled](%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%20f34168dd83c34d61a19390d293ab5d76/Untitled%205.png)

![Untitled](%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%20f34168dd83c34d61a19390d293ab5d76/Untitled%206.png)

！这个直观的解释不是特别让人明白是什么意思。。。

理论解释：

![Untitled](%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%20f34168dd83c34d61a19390d293ab5d76/Untitled%207.png)

从某一点到最低点的最佳距离是一次微分/二次微分

![Untitled](%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%20f34168dd83c34d61a19390d293ab5d76/Untitled%208.png)

而这个算法除数位置就是模拟二次微分的值，同时避免增加过多的计算而增加时间消耗。

## 3、随机梯度下降法

![Untitled](%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%20f34168dd83c34d61a19390d293ab5d76/Untitled%209.png)

梯度下降是计算出所有样例再更新参数，随机梯度是观察部分样例立马更新参数。更新次数更多，

## 4、特征缩放

![Untitled](%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%20f34168dd83c34d61a19390d293ab5d76/Untitled%2010.png)

不同特征的分布不相同会导致对结果的影响不同，缩放他们使得他们在同一个分布范围下，同组学习率就容易处理，因为更新他们参数值相同时，所产生的影响不同。

![Untitled](%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%20f34168dd83c34d61a19390d293ab5d76/Untitled%2011.png)